{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3e2275-df09-4c2e-b383-0152783cf693",
   "metadata": {
    "tags": []
   },
   "source": [
    "<font size=\"12\">Snow and Cloud Classification in Historical SPOT Images: An Image Emulation Approach for Training a Deep Learning Model Without Reference Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166414fb-8b75-416c-8615-e67e25e596d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = #local directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbea10-d4e9-42cb-9017-5a4f967b864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) CREATE TRAINING DATASET FOR THE UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856811a5-13f1-4d2b-bb7d-505370c209ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.1) CREATE A GKDE DISTRIBUTION OF SWH SATURATION AND MINIMUM REFLECTANCE VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb756d5-efd3-43db-931c-6ee364124d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A.1.1) GET CSV OF SATURATION AND MINIMUM REFLECTANCE VALUES FROM SWH USING S3FS\n",
    "#use kernel s3-env\n",
    "#if s3fs generate errors, try running the command 'kinit' (followed by pwd) in the console before trying again.\n",
    "import os\n",
    "import json\n",
    "\n",
    "area = \"PYR\"\n",
    "os.system(f\"mkdir -p {project_dir}/DATA/SWH_SATURATIONS/{area}\")\n",
    "bbox= [48,257,55,262] # bounding box using the SWH path/row (K,J) tiling system [Kmin,Jmin,Kmax,Jmax]\n",
    "                             # Pyrenees: [36,263,45,265]; French Alps: [48,257,55,262]\n",
    "start_year= 1986\n",
    "end_year = 2015\n",
    "bw= 0.1# bw_method (parameter used by the scipy gaussian_kde tool)\n",
    "\n",
    "\n",
    "\n",
    "jb_script=f\"{project_dir}/CODE/trex/get_swh_saturation_job.sh\"\n",
    "\n",
    "for year in range(start_year,end_year+1):\n",
    "    common_name = f\"{year}_{bbox[0]}-{bbox[2]}_{bbox[1]}-{bbox[3]}\"\n",
    "    config = {\n",
    "        \"work_dir\":project_dir+\"/DATA/SWH_SATURATIONS/\"+area,\n",
    "        \"bbox\": bbox,\n",
    "        \"start_date\": f\"{year}0101\", \n",
    "        \"end_date\": f\"{year}1231\", \n",
    "    }\n",
    "\n",
    "    with open(f\"{project_dir}/CODE/config/get_swh_saturation_{common_name}.config\", \"w\") as outfile:\n",
    "        json.dump(config, outfile)\n",
    "\n",
    "    export =  \",\".join(\n",
    "        [\n",
    "            f\"config_file=\\\"{project_dir}/CODE/config/get_swh_saturation_{common_name}.config\\\"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    job_common_params =  \" \".join(\n",
    "        [\n",
    "            \"-A\",\"cnes_level2\", # cesbio or campus with max jobs 1000 and maxsubmit 10000 ; cnes_level2 with maxjobs 500 and maxsubmit 5000\n",
    "            \" --job-name=SWH_SAT\",\n",
    "            \" --time=00:59:59\",\n",
    "            \"-N\",\"1\",\"-n\",\"1\",\n",
    "            \"--mem=1G\",\n",
    "            \"-o\",f\"{project_dir}/CODE/logs/LOG_SWH_SAT_JOB_{common_name}\",\n",
    "            f\"--export={export}\",jb_script\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    os.system(\"sbatch \"+job_common_params)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368652c-8a70-4e11-aa8e-08a1db9a99b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A.1.2) GENERATE GKDE from CSVs\n",
    "import os\n",
    "import json\n",
    "\n",
    "area = \"PYR\"\n",
    "jb_script=f\"{project_dir}/CODE/trex/get_swh_gkde_job.sh\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"work_dir\":f\"{project_dir}/DATA/SWH_SATURATIONS/{area}\",\n",
    "    \"bw\": 0.1\n",
    "}\n",
    "\n",
    "with open(f\"{project_dir}/CODE/config/get_swh_GKDE.config\", \"w\") as outfile:\n",
    "    json.dump(config, outfile)\n",
    "\n",
    "export =  \",\".join(\n",
    "    [\n",
    "        f\"config_file=\\\"{project_dir}/CODE/config/get_swh_GKDE.config\\\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "job_common_params =  \" \".join(\n",
    "    [\n",
    "        \"-A\",\"cnes_level2\", # cesbio or campus with max jobs 1000 and maxsubmit 10000 ; cnes_level2 with maxjobs 500 and maxsubmit 5000\n",
    "        \" --job-name=SWH_GKDE\",\n",
    "        \" --time=00:59:59\",\n",
    "        \"-N\",\"1\",\"-n\",\"1\",\n",
    "        \"--mem=1G\",\n",
    "        \"-o\",f\"{project_dir}/CODE/logs/LOG_SWH_GKDE_JOB\",\n",
    "        f\"--export={export}\",jb_script\n",
    "    ]\n",
    ")\n",
    "\n",
    "os.system(\"sbatch \"+job_common_params)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33343911-f982-46ca-a5bc-ad80dd317453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.1.3) Generate the saturation graph used for the paper\n",
    "df_SPOT4= # get the dataframe produced by 1.1.1)\n",
    "df_SPOT4['XS1 RANGE'] = np.where(df_SPOT4['XS1_SAT']<0.2 , '< 0.2',\n",
    "                         np.where(df_SPOT4['XS1_SAT']<0.3, '0.2 - 0.3',\n",
    "                          np.where(df_SPOT4['XS1_SAT']<0.4, '0.3 - 0.4',\n",
    "                           np.where(df_SPOT4['XS1_SAT']<0.5, '0.4 - 0.5','>= 0.5'))))\n",
    "df_SPOT4['XS2 RANGE'] = np.where(df_SPOT4['XS2_SAT']<0.2 , '< 0.2',\n",
    "                         np.where(df_SPOT4['XS2_SAT']<0.3, '0.2 - 0.3',\n",
    "                          np.where(df_SPOT4['XS2_SAT']<0.4, '0.3 - 0.4',\n",
    "                           np.where(df_SPOT4['XS2_SAT']<0.5, '0.4 - 0.5','>= 0.5')))) \n",
    "df_SPOT4['XS3 RANGE'] = np.where(df_SPOT4['XS3_SAT']<0.4 , '< 0.4',\n",
    "                         np.where(df_SPOT4['XS3_SAT']<0.5, '0.4 - 0.5',\n",
    "                          np.where(df_SPOT4['XS3_SAT']<0.6, '0.5 - 0.6',\n",
    "                           np.where(df_SPOT4['XS3_SAT']<0.7, '0.6 - 0.7','>= 0.7'))))\n",
    "# gaussian KDE for 3 bands\n",
    "XS1 = df_SPOT4['XS1_SAT']\n",
    "XS2 = df_SPOT4['XS2_SAT']\n",
    "XS3 = df_SPOT4['XS3_SAT']\n",
    "XS1_min=XS1.min()-0.1\n",
    "XS1_max=XS1.max()+0.1\n",
    "XS2_min=XS2.min()-0.1\n",
    "XS2_max=XS2.max()+0.1\n",
    "XS3_min=XS3.min()-0.1\n",
    "XS3_max=XS3.max()+0.1\n",
    "val_XS123 = np.vstack([XS1, XS2,XS3])\n",
    "kernel_XS123 = GKDE(val_XS123,bw_method=0.09)\n",
    "SXS1, SXS2,SXS3 = kernel_XS123.resample(size=500)\n",
    "df_GKDE = pd.DataFrame(data={'XS1_SAT': SXS1, 'XS2_SAT': SXS2,'XS3_SAT': SXS3}) \n",
    "df_GKDE['XS1 RANGE'] = np.where(df_GKDE['XS1_SAT']<0.2 , '< 0.2',\n",
    "                         np.where(df_GKDE['XS1_SAT']<0.3, '0.2 - 0.3',\n",
    "                          np.where(df_GKDE['XS1_SAT']<0.4, '0.3 - 0.4',\n",
    "                           np.where(df_GKDE['XS1_SAT']<0.5, '0.4 - 0.5','>= 0.5'))))\n",
    "df_GKDE['XS2 RANGE'] = np.where(df_GKDE['XS2_SAT']<0.2 , '< 0.2',\n",
    "                         np.where(df_GKDE['XS2_SAT']<0.3, '0.2 - 0.3',\n",
    "                          np.where(df_GKDE['XS2_SAT']<0.4, '0.3 - 0.4',\n",
    "                           np.where(df_GKDE['XS2_SAT']<0.5, '0.4 - 0.5','>= 0.5')))) \n",
    "df_GKDE['XS3 RANGE'] = np.where(df_GKDE['XS3_SAT']<0.4 , '< 0.4',\n",
    "                         np.where(df_GKDE['XS3_SAT']<0.5, '0.4 - 0.5',\n",
    "                          np.where(df_GKDE['XS3_SAT']<0.6, '0.5 - 0.6',\n",
    "                           np.where(df_GKDE['XS3_SAT']<0.7, '0.6 - 0.7','>= 0.7'))))\n",
    "# gaussian KDE fro 2 bands for graph positions\n",
    "grid_xs1, grid_xs2  = np.mgrid[0:XS1_max:100j, 0:XS2_max:100j]\n",
    "pos_XS12 = np.vstack([grid_xs1.ravel(), grid_xs2.ravel()])\n",
    "val_XS12 = np.vstack([XS1, XS2])\n",
    "kernel_XS12 = GKDE(val_XS12,bw_method=0.1)\n",
    "Z_XS12 = np.reshape(kernel_XS12(pos_XS12).T, grid_xs1.shape)\n",
    "#['< 0.4','0.4 - 0.5','0.5 - 0.6','0.6 - 0.7','>= 0.7'] \n",
    "#['< 0.2','0.2 - 0.3','0.3 - 0.4','0.4 - 0.5','>= 0.5'] \n",
    "fig, axs = plt.subplots(ncols=3,sharey=True)\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(6)\n",
    "sns.scatterplot(x='XS2_SAT', y=\"XS3_SAT\", hue = 'XS1 RANGE',hue_order= ['< 0.2','0.2 - 0.3','0.3 - 0.4','0.4 - 0.5','>= 0.5']  ,  palette=\"Set2\",data=df_SPOT4, ax=axs[0], s=30)\n",
    "axs[0].set_xlim([0, XS2_max])\n",
    "axs[0].set_title(\"SPOT 4 THEIA \\n reflectance saturations\",size=14)\n",
    "axs[0].set_xlabel(\"Red saturations\",size=14)\n",
    "axs[0].set_ylabel(\"NIR saturations\",size=14)\n",
    "axs[0].set_ylim([0.2, XS3_max])\n",
    "axs[0].tick_params(axis='both', labelsize=11, rotation=45)\n",
    "#axs[0].tick_params(axis='y', labelsize=12)\n",
    "axs[0].legend(title=\"Green saturations\")\n",
    "#gkde = axs[1].imshow(np.rot90(Z_XS12), cmap=plt.cm.gist_earth_r,extent=[0, XS1_max, 0, XS2_max],aspect=\"auto\")\n",
    "#cbaxes = inset_axes(axs[1], width=\"3%\", height=\"50%\", loc=\"upper right\") \n",
    "sns.kdeplot(x='XS2_SAT', y=\"XS3_SAT\",data=df_SPOT4, common_norm=True,ax=axs[1], bw_method = 0.09, fill=True)\n",
    "axs[1].set_xlim([0, XS2_max])\n",
    "axs[1].set_title(\"GKDE \\n probability density function\",size=14)\n",
    "axs[1].set_xlabel(\"Red saturations\",size=14)\n",
    "axs[1].tick_params(axis='both', labelsize=11, rotation=45)\n",
    "#axs[1].set_ylabel(\"NIR SATURATION REFL\",size=14)\n",
    "axs[1].set_ylim([0.2, XS3_max])\n",
    "#fig.colorbar(gkde, ax=axs[1],shrink=0.6, cax=cbaxes)\n",
    "sns.scatterplot(x='XS2_SAT', y=\"XS3_SAT\", hue = 'XS1 RANGE',hue_order=  ['< 0.2','0.2 - 0.3','0.3 - 0.4','0.4 - 0.5','>= 0.5']  , palette=\"Set2\",data=df_GKDE, ax=axs[2], s=30)\n",
    "axs[2].set_xlim([0, XS2_max])\n",
    "axs[2].set_title(\"GKDE \\n sampling = 500\",size=14)\n",
    "axs[2].set_xlabel(\"Red saturations\",size=14)\n",
    "#axs[2].set_ylabel(\"NIR SATURATION REFL\",size=14)\n",
    "axs[2].set_ylim([0.2, XS3_max])\n",
    "axs[2].tick_params(axis='both', labelsize=11, rotation=45)\n",
    "axs[2].legend(title=\"Green saturations\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"spot4GKDE.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec5436-07a1-4587-a6ae-e50e878b9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.2) DOWNLOAD SENTINEL-2 DATA (L1C REFLECTANCE, MAJA CLOUD MASKS, SNOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc7e10-9536-4436-a298-0bdab1385578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import pathlib\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538503b-1b76-4f52-a895-1b4205fa7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.2.1) USE AMALTHEE TO FIND APPROPRIATE S2 L1C, L2A AND SNOW PRODUCTS\n",
    "#use kernel Amalthee_TREX\n",
    "# in this example we look for Sentinel-2 data over the pyrenees\n",
    "from libamalthee import Amalthee\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "cloud_range = \"[0,90]\"\n",
    "start_date = \"2016-01-01\"\n",
    "end_date=\"2019-12-31\"\n",
    "\n",
    "\n",
    "\n",
    "tiles=[[\"30TXN\",\"94\"],[\"30TYN\",\"51\"],[\"31TCH\",\"51\"],[\"31TDH\",\"8\"]] #for the Pyrenees: [tile name,relative orbit number]\n",
    "\n",
    "df_list = []\n",
    "for tile,ron in tiles:\n",
    "    \n",
    "    #L1C\n",
    "    parameters = {\"processingLevel\": \"LEVEL1C\", \"tileid\": tile,\"cloudCover\":cloud_range,\"relativeOrbitNumber\":ron}\n",
    "    amalthee_peps = Amalthee('peps')\n",
    "    amalthee_peps.search(\"S2ST\",f\"{start_date}\",f\"{end_date}\",parameters)\n",
    "    df_L1C = amalthee_peps.products\n",
    "    print(\"L1C\",len(df_L1C))\n",
    "    df_L1C['datetime'] =  pd.to_datetime(df_L1C['acquisition_date'])\n",
    "    df_L1C['month'] = df_L1C['datetime'].dt.strftime('%m')\n",
    "    df_L1C['day'] = df_L1C['datetime'].dt.strftime('%d')\n",
    "    df_L1C['year'] = df_L1C['datetime'].dt.strftime('%Y')\n",
    "    df_L1C['tile'] = tile\n",
    "    df_L1C[\"ID\"] = df_L1C.index\n",
    "    df_L1C[\"LEVEL\"] = \"L1C\"\n",
    "    df_L1C[\"area\"] = \"PYR\"\n",
    "    df_list.append(df_L1C)\n",
    "    #L2A\n",
    "    parameters = {\"processingLevel\": \"LEVEL2A\", \"location\": \"T\"+tile,\"cloudCover\":cloud_range,\"relativeOrbitNumber\":ron}\n",
    "    amalthee_theia = Amalthee('theia')\n",
    "    amalthee_theia.search(\"SENTINEL2\",f\"{start_date}\",f\"{end_date}\",parameters)\n",
    "    df_L2A = amalthee_theia.products\n",
    "    print(\"L2A\",len(df_L2A))\n",
    "    df_L2A['datetime'] =  pd.to_datetime(df_L2A['acquisition_date'],format='%Y%m%d-%H%M%S-%f')\n",
    "    df_L2A['month'] = df_L2A['datetime'].dt.strftime('%m')\n",
    "    df_L2A['day'] = df_L2A['datetime'].dt.strftime('%d')\n",
    "    df_L2A['year'] = df_L2A['datetime'].dt.strftime('%Y')\n",
    "    df_L2A['tile'] = tile\n",
    "    df_L2A[\"ID\"] = df_L2A.index\n",
    "    df_L2A[\"LEVEL\"] = \"L2A\"\n",
    "    df_L2A[\"area\"] = \"PYR\"\n",
    "    df_list.append(df_L2A)\n",
    "    #Snow\n",
    "    parameters = {\"processingLevel\": \"L2B-SNOW\", \"location\": \"T\"+tile,\"cloudCover\":cloud_range,\"relativeOrbitNumber\":ron}\n",
    "    amalthee_theia = Amalthee('theia')\n",
    "    amalthee_theia.search(\"Snow\",f\"{start_date}\",f\"{end_date}\",parameters)\n",
    "    df_SNOW = amalthee_theia.products\n",
    "    print(\"L2B\",len(df_SNOW))\n",
    "    df_SNOW['datetime'] =  pd.to_datetime(df_SNOW['acquisition_date'],format='%Y%m%d-%H%M%S-%f')\n",
    "    df_SNOW['month'] = df_SNOW['datetime'].dt.strftime('%m')\n",
    "    df_SNOW['day'] = df_SNOW['datetime'].dt.strftime('%d')\n",
    "    df_SNOW['year'] = df_SNOW['datetime'].dt.strftime('%Y')\n",
    "    df_SNOW['tile'] = tile\n",
    "    df_SNOW[\"ID\"] = df_SNOW.index\n",
    "    df_SNOW[\"LEVEL\"] = \"L2B\"\n",
    "    df_SNOW[\"area\"] = \"PYR\"\n",
    "    df_list.append(df_SNOW)\n",
    "    \n",
    "    \n",
    "\n",
    "tiles=[[\"31TGM\",\"108\"],[\"31TGL\",\"108\"],[\"31TGK\",\"108\"],[\"31TGJ\",\"108\"],[\"32TLS\",\"108\"],[\"32TLR\",\"108\"],[\"32TLQ\",\"108\"],[\"32TLP\",\"108\"]] # for the french Alps\n",
    "for tile,ron in tiles:\n",
    "    \n",
    "    #L1C\n",
    "    parameters = {\"processingLevel\": \"LEVEL1C\", \"tileid\": tile,\"cloudCover\":cloud_range,\"relativeOrbitNumber\":ron}\n",
    "    amalthee_peps = Amalthee('peps')\n",
    "    amalthee_peps.search(\"S2ST\",f\"{start_date}\",f\"{end_date}\",parameters)\n",
    "    df_L1C = amalthee_peps.products\n",
    "    print(\"L1C\",len(df_L1C))\n",
    "    df_L1C['datetime'] =  pd.to_datetime(df_L1C['acquisition_date'])\n",
    "    df_L1C['month'] = df_L1C['datetime'].dt.strftime('%m')\n",
    "    df_L1C['day'] = df_L1C['datetime'].dt.strftime('%d')\n",
    "    df_L1C['year'] = df_L1C['datetime'].dt.strftime('%Y')\n",
    "    df_L1C['tile'] = tile\n",
    "    df_L1C[\"ID\"] = df_L1C.index\n",
    "    df_L1C[\"LEVEL\"] = \"L1C\"\n",
    "    df_L1C[\"area\"] = \"ALP\"\n",
    "    df_list.append(df_L1C)\n",
    "    #L2A\n",
    "    parameters = {\"processingLevel\": \"LEVEL2A\", \"location\": \"T\"+tile,\"cloudCover\":cloud_range,\"relativeOrbitNumber\":ron}\n",
    "    amalthee_theia = Amalthee('theia')\n",
    "    amalthee_theia.search(\"SENTINEL2\",f\"{start_date}\",f\"{end_date}\",parameters)\n",
    "    df_L2A = amalthee_theia.products\n",
    "    print(\"L2A\",len(df_L2A))\n",
    "    df_L2A['datetime'] =  pd.to_datetime(df_L2A['acquisition_date'],format='%Y%m%d-%H%M%S-%f')\n",
    "    df_L2A['month'] = df_L2A['datetime'].dt.strftime('%m')\n",
    "    df_L2A['day'] = df_L2A['datetime'].dt.strftime('%d')\n",
    "    df_L2A['year'] = df_L2A['datetime'].dt.strftime('%Y')\n",
    "    df_L2A['tile'] = tile\n",
    "    df_L2A[\"ID\"] = df_L2A.index\n",
    "    df_L2A[\"LEVEL\"] = \"L2A\"\n",
    "    df_L2A[\"area\"] = \"ALP\"\n",
    "    df_list.append(df_L2A)\n",
    "    #Snow\n",
    "    parameters = {\"processingLevel\": \"L2B-SNOW\", \"location\": \"T\"+tile,\"cloudCover\":cloud_range,\"relativeOrbitNumber\":ron}\n",
    "    amalthee_theia = Amalthee('theia')\n",
    "    amalthee_theia.search(\"Snow\",f\"{start_date}\",f\"{end_date}\",parameters)\n",
    "    df_SNOW = amalthee_theia.products\n",
    "    print(\"L2B\",len(df_SNOW))\n",
    "    df_SNOW['datetime'] =  pd.to_datetime(df_SNOW['acquisition_date'],format='%Y%m%d-%H%M%S-%f')\n",
    "    df_SNOW['month'] = df_SNOW['datetime'].dt.strftime('%m')\n",
    "    df_SNOW['day'] = df_SNOW['datetime'].dt.strftime('%d')\n",
    "    df_SNOW['year'] = df_SNOW['datetime'].dt.strftime('%Y')\n",
    "    df_SNOW['tile'] = tile\n",
    "    df_SNOW[\"ID\"] = df_SNOW.index\n",
    "    df_SNOW[\"LEVEL\"] = \"L2B\"\n",
    "    df_SNOW[\"area\"] = \"ALP\"\n",
    "    df_list.append(df_SNOW)\n",
    "    \n",
    "\n",
    "#concatenate the dataframes into one dataframe referencing one S2 scene per tile and per month of the year\n",
    "\n",
    "df = pd.concat(df_list).sample(frac=1).drop_duplicates(subset=['day','month','year','tile','area','LEVEL'])\n",
    "df = df.pivot(index=[\"year\", \"month\",'day','tile','area'], columns=[\"LEVEL\"], values=\"ID\")\n",
    "df =df.dropna().reset_index()\n",
    "df = df.sample(frac=1).drop_duplicates(subset=['day','month','year','area'])\n",
    "df = df.groupby(['tile','month']).sample(1)\n",
    "df['L1C_PATH'] = \"/work/datalake/S2-L1C/\" + df['tile'] + \"/\"+ df['year'] + \"/\" + df['month'] + \"/\"+ df['day'] + \"/\"+ df[f'L1C']+\".SAFE\"\n",
    "df['L2A_PATH'] = \"/work/datalake/S2-L2A-THEIA/\" + df['tile'] + \"/\"+ df['year'] + \"/\" + df['month'] + \"/\"+ df['day'] + \"/\"+ df['L2A']\n",
    "df['L2B_PATH'] = project_dir+\"/DATA/S2/L2B/\" + df['tile'] + \"/\"+ df['year'] + \"/\" + df['month'] + \"/\"+ df['day'] + \"/\"+ df['L2B']\n",
    "df['INSTRUMENT']  = \"MSI\"\n",
    "df['SATELLITE'] = df.L2B.str[:10]\n",
    "\n",
    "\n",
    "os.system(f\"mkdir -p {project_dir}/CSVS\")\n",
    "product_csv = \"S2_PRODUCTS.csv\" \n",
    "df.to_csv(os.path.join(project_dir,\"CSVS\",product_csv), index=False)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44a4f0-c25c-4772-a173-4ff063c2b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.2.2) USE AMALTHEE TO DOWNLOAD FOUND S2 L1C AND L2A PRODUCTS\n",
    "#use kernel Amalthee_TREX\n",
    "# amalthee will fill the datalake with the missing L1C and L2A products\n",
    "from libamalthee import Amalthee\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "product_csv = \"S2_PRODUCTS.csv\" \n",
    "df = pd.read_csv(os.path.join(project_dir,\"CSVS\",product_csv), dtype = str)\n",
    "\n",
    "#L1C\n",
    "amalthee_peps = Amalthee('peps')\n",
    "amalthee_peps.add(df.L1C)\n",
    "amalthee_peps.fill_datalake(user_email=\"zacharie.barrou-dumont@univ-tlse3.fr\")\n",
    "\n",
    "#L2A\n",
    "amalthee_theia = Amalthee('theia')\n",
    "amalthee_theia.add(df.L2A)\n",
    "amalthee_theia.fill_datalake(user_email=\"zacharie.barrou-dumont@univ-tlse3.fr\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcdb91-f3e7-4278-9e9a-affc84c5d0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#A.2.3) USE S3FS TO DOWNLOAD FOUND L2B SNOW PRODUCTS (because can't be downloaded using amalthee for some reason)\n",
    "#use kernel s3-env\n",
    "#if s3fs generate errors, try running the command 'kinit' (followed by pwd) in the console before trying again.\n",
    "from assumerole import assumerole\n",
    "import s3fs\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "product_csv = \"S2_PRODUCTS.csv\" \n",
    "df = pd.read_csv(os.path.join(project_dir,\"CSVS\",product_csv), dtype = str)\n",
    "\n",
    "\n",
    "ENDPOINT_URL=\"https://s3.datalake.cnes.fr\"\n",
    "credentials = assumerole.getCredentials(\"arn:aws:iam::732885638740:role/public-read-only-OT\", Duration=7200)\n",
    "s3 = s3fs.S3FileSystem(\n",
    "      client_kwargs={\n",
    "                      'aws_access_key_id': credentials['AWS_ACCESS_KEY_ID'],\n",
    "                \n",
    "          'aws_secret_access_key': credentials['AWS_SECRET_ACCESS_KEY'],\n",
    "                      'aws_session_token': credentials['AWS_SESSION_TOKEN'],\n",
    "         'endpoint_url': 'https://s3.datalake.cnes.fr'\n",
    "      }\n",
    "   )\n",
    "l = []\n",
    "for index, row in df.iterrows():\n",
    "    print(row['L2B_PATH'])\n",
    "    print(f\"muscate/Snow/{row['year']}/{row['month']}/{row['day']}/{row['L2B']}/{row['L2B']}.zip\")\n",
    "    print(f\"{row['L2B_PATH']}/{row['L2B']}.zip\")\n",
    "    os.system(f\"mkdir -p {row['L2B_PATH']}\")\n",
    "    s3_path = f\"muscate/Snow/{row['year']}/{row['month']}/{row['day']}/{row['L2B']}/{row['L2B']}.zip\"\n",
    "    s3.get(s3_path,row['L2B_PATH'])\n",
    "    os.system(f\"unzip {row['L2B_PATH']}/{row['L2B']}.zip -d {row['L2B_PATH']}\")\n",
    "    os.system(f\"rm {row['L2B_PATH']}/{row['L2B']}.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f748d6e-0814-484b-96c6-6be5186b4115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#A.3) GET AUXILIARY DATA  (DEM, HILLSHADE, MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad2e7d-6c93-43c3-a98f-d96942b80c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.3.1) SET AUXILIARY DATA PATH \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "product_csv = \"S2_PRODUCTS.csv\"\n",
    "df = pd.read_csv(os.path.join(project_dir,\"CSVS\",product_csv), dtype = str)\n",
    "\n",
    "df['DEM_PATH'] = f\"/work/datalake/static_aux/MNT/COP-DEM_GLO-30-DGED_S2_tiles/COP-DEM_GLO-30-DGED_\"+df['tile']+\".tif\" \n",
    "df['TCD_PATH'] = f\"/work/datalake/static_aux/TreeCoverDensity/\"+df['tile']+\"/TCD_\"+df['tile']+\".tif\" \n",
    "df['WATER_PATH'] = f\"/work/datalake/static_aux/MASQUES/eu_hydro/raster/20m/\"+df['tile']+\"/eu_hydro_20m_\"+df['tile']+\".tif\" \n",
    "df['HILL_PATH'] = project_dir+\"/DATA/S2/HILLSHADE/\"+df['tile']+\"/\"+df['year']+\"/\"+df['month']+\"/\"+df['day']+\"/HILL_\"+df['year']+df['month']+df['day']+\"_\"+df['tile']+\".tif\"\n",
    "df['MASK_PATH'] = project_dir+\"/DATA/S2/MASK/\"+df['tile']+\"/\"+df['year']+\"/\"+df['month']+\"/\"+df['day']+\"/MASK_\"+df['year']+df['month']+df['day']+\"_\"+df['tile']+\".tif\"\n",
    "df.to_csv(os.path.join(project_dir,\"CSVS\",product_csv), index=False)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f6a4c-f708-4444-af39-44dc70ad3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.3.2) CREATE HILLSHADES \n",
    "\n",
    "import os\n",
    "\n",
    "product_csv = \"S2_PRODUCTS.csv\"\n",
    "jb_script=project_dir+\"/CODE/trex/get_hillshade_job.sh\"\n",
    "export =  \",\".join(\n",
    "    [\n",
    "        f\"csv_file=\\\"{project_dir+'/CSVS/'+product_csv}\\\"\"\n",
    "    ]\n",
    ")\n",
    "job_common_params =  \" \".join(\n",
    "    [\n",
    "        \"-A\",\"cnes_level2\", # cesbio or campus with max jobs 1000 and maxsubmit 10000 ; cnes_level2 with maxjobs 500 and maxsubmit 5000\n",
    "        \" --job-name=HILLSHADE\",\n",
    "        \" --time=00:04:59\",\n",
    "        \"-N\",\"1\",\"-n\",\"1\",\n",
    "        \"--mem=1G\",\n",
    "        \"-o\",f\"{project_dir}/CODE/logs/LOG_HILLSHADE\",\n",
    "        f\"--export={export}\",jb_script\n",
    "    ]\n",
    ")\n",
    "os.system(\"sbatch \"+job_common_params)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2baf6-ac00-49ba-a1a7-e174bbd93724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.3.3) CREATE MASKS\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "product_csv = \"S2_PRODUCTS.csv\"\n",
    "jb_script=project_dir+\"/CODE/trex/get_mask_job.sh\"\n",
    "export =  \",\".join(\n",
    "    [\n",
    "        f\"csv_file=\\\"{project_dir+'/CSVS/'+product_csv}\\\"\"\n",
    "    ]\n",
    ")\n",
    "job_common_params =  \" \".join(\n",
    "    [\n",
    "        \"-A\",\"cnes_level2\", # cesbio or campus with max jobs 1000 and maxsubmit 10000 ; cnes_level2 with maxjobs 500 and maxsubmit 5000\n",
    "        \" --job-name=MASK\",\n",
    "        \" --time=00:04:59\",\n",
    "        \"-N\",\"1\",\"-n\",\"1\",\n",
    "        \"--mem=1G\",\n",
    "        \"-o\",f\"{project_dir}/CODE/logs/LOG_MASK\",\n",
    "        f\"--export={export}\",jb_script\n",
    "    ]\n",
    ")\n",
    "  \n",
    "os.system(\"sbatch \"+job_common_params)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583293d-eb0d-4d3c-875a-ebe4863016e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.4) SPLIT DATA INTO PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a94ac-e1b2-4ec4-81c7-7d33d553fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.4.1) MAKE PATCHES \n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "product_csv = \"S2_PRODUCTS.csv\"\n",
    "os.system(f\"mkdir -p {project_dir}/TMP\")\n",
    "os.system(f\"rm {project_dir}/TMP/*\")\n",
    "\n",
    "pixel_res= 20\n",
    "patch_size = 572\n",
    "overlap = 92\n",
    "\n",
    "df = pd.read_csv(csv_path, dtype = str)\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    jb_script=project_dir+\"/CODE/trex/get_patches_job.sh\"\n",
    "\n",
    "\n",
    "    config = {\n",
    "        \"input_csv\": os.path.join(project_dir,\"CSVS\",product_csv),\n",
    "        \"index\":index,\n",
    "        \"project_dir\": project_dir,\n",
    "        \"patch_size\" : patch_size,\n",
    "        \"overlap\": overlap,\n",
    "        \"res\": pixel_res,\n",
    "    }\n",
    "\n",
    "    with open(f\"{project_dir}/CODE/config/get_patches_job_{index}.config\", \"w\") as outfile:\n",
    "        json.dump(config, outfile)\n",
    "\n",
    "    export =  \",\".join(\n",
    "        [\n",
    "            f\"config_file=\\\"{project_dir}/CODE/config/get_patches_job_{index}.config\\\"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    job_common_params =  \" \".join(\n",
    "        [\n",
    "            \"-A\",\"cnes_level2\", # cesbio or campus with max jobs 1000 and maxsubmit 10000 ; cnes_level2 with maxjobs 500 and maxsubmit 5000\n",
    "            f\" --job-name=P{index}\",\n",
    "            \" --time=00:59:59\",\n",
    "            \"-N\",\"1\",\"-n\",\"4\",\n",
    "            \"--mem=10G\",\n",
    "            \"-o\",f\"{project_dir}/CODE/logs/LOG_PATCHES_{index}_JOB\",\n",
    "            f\"--export={export}\",jb_script\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    os.system(\"sbatch \"+job_common_params)   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e870e-abdc-4343-a0ec-4d0a81f2aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.4.2) COMBINE PATCH DATAFRAMES\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "list_df = []\n",
    "for i,patch in enumerate(sorted(glob.glob(os.path.join(project_dir,\"TMP\",\"PATCHES_*.csv\")))):\n",
    "    p = pd.read_csv(patch, dtype = str)\n",
    "    p['i'] = i\n",
    "    list_df.append(p)\n",
    "    \n",
    "df_patchs = pd.concat(list_df)\n",
    "\n",
    "df_patchs = df_patchs.pivot(index=['i',\"year\", \"month\",'day','tile','area','row','col','SATELLITE','INSTRUMENT'], columns=[\"band\"], values=\"path\")\n",
    "\n",
    "\n",
    "df_patchs =df_patchs.dropna().reset_index()\n",
    "\n",
    "#on retire la derniere ligne et colonne car de tailles plus petites\n",
    "df_patchs =  df_patchs[(df_patchs.row != df_patchs['row'].max()) & (df_patchs.col != df_patchs['col'].max())]\n",
    "\n",
    "\n",
    "df_patchs.to_csv(os.path.join(project_dir,\"CSVS\",f\"PATCHES.csv\"), index=False) \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aeb683-7d77-4166-a5db-ff7e3a5879f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.5) SPOTIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c57383-c5de-4d02-a4d5-974e7c8d3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.5.1) MAKE SPOTIFY PATCHES\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.system(f\"rm {project_dir}/TMP/*\")\n",
    "product_csv = \"PATCHES.csv\"\n",
    "csv_path = os.path.join(project_dir,\"CSVS\",product_csv)\n",
    "df = pd.read_csv(csv_path, dtype = str)\n",
    "jb_script=project_dir+\"/CODE/trex/spotify_job.sh\"\n",
    "library = project_dir+\"/CODE/satellite_library.json\"\n",
    "draws=1\n",
    "add_SAT = True\n",
    "for i in sorted(df.i.unique()):\n",
    "    for target_sat,target_ins in [(\"SPOT1\",\"HRV1\"),(\"SPOT2\",\"HRV1\"),(\"SPOT3\",\"HRV1\"),(\"SPOT4\",\"HRVIR1\"),(\"SPOT5\",\"HRG1\")]:\n",
    "    \n",
    "    \n",
    "\n",
    "        config = {\n",
    "            \"input_csv\": csv_path,\n",
    "            \"index\":i,\n",
    "            \"target_sat\":target_sat,\n",
    "            \"target_ins\":target_ins,\n",
    "            \"draws\":draws,\n",
    "            \"add_sat\":add_SAT,\n",
    "            \"work_dir\": project_dir,\n",
    "            \"library\":library,\n",
    "        }\n",
    "\n",
    "        with open(f\"{project_dir}/CODE/config/spotify_job_{i}_{target_sat}_{target_ins}.config\", \"w\") as outfile:\n",
    "            json.dump(config, outfile)\n",
    "\n",
    "        export =  \",\".join(\n",
    "            [\n",
    "                f\"config_file=\\\"{project_dir}/CODE/config/spotify_job_{i}_{target_sat}_{target_ins}.config\\\"\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        job_common_params =  \" \".join(\n",
    "            [\n",
    "                \"-A\",\"cnes_level2\", # cesbio or campus with max jobs 1000 and maxsubmit 10000 ; cnes_level2 with maxjobs 500 and maxsubmit 5000\n",
    "                f\" --job-name=S{i}_{target_sat}_{target_ins}\",\n",
    "                \" --time=00:59:59\",\n",
    "                \"-N\",\"1\",\"-n\",\"4\",\n",
    "                \"--mem=10G\",\n",
    "                \"-o\",f\"{project_dir}/CODE/logs/LOG_SPOTIFY_{i}_{target_sat}_{target_ins}_JOB\",\n",
    "                f\"--export={export}\",jb_script\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        os.system(\"sbatch \"+job_common_params)   \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d550b-55fe-4c53-9512-c4ab2641fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.4.2) COMBINE SPOTIFY PATCH DATAFRAMES\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "list_df = []\n",
    "for i,patch in enumerate(sorted(glob.glob(os.path.join(project_dir,\"TMP\",\"PATCHES_*.csv\")))):\n",
    "    p = pd.read_csv(patch, dtype = str)\n",
    "    p['i'] = i\n",
    "    list_df.append(p)\n",
    "    \n",
    "df_patchs = pd.concat(list_df)\n",
    "\n",
    "#df_patchs =df_patchs.dropna().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "df_patchs.to_csv(os.path.join(project_dir,\"CSVS\",f\"PATCHES_PSEUDO.csv\"), index=False) \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cebcb4-20c2-4905-91cc-1e4d1fa6af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.6) TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677913c-7f45-4ff0-80fc-3fb14bc563b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.6.1) REMOVE PATCHES WITH MEAN ELEVATION < 1200 m\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from osgeo import osr, gdal\n",
    "from osgeo.gdalnumeric import *\n",
    "from osgeo.gdalconst import *\n",
    "\n",
    "\n",
    "product_csv = \"PATCHES_PSEUDO.csv\"\n",
    "csv_path = os.path.join(project_dir,\"CSVS\",product_csv)\n",
    "df = pd.read_csv(csv_path, dtype = str)\n",
    "filtered_df = df.copy(deep=True)\n",
    "\n",
    "for i, dfi in df.groupby(['area','tile', 'row', 'col']):\n",
    "    raster = gdal.Open(dfi['DEM'].iloc[0])\n",
    "    array = BandReadAsArray(raster.GetRasterBand(1))\n",
    "    avg = np.mean(array)\n",
    "    if avg < 1200 :\n",
    "        print(i)\n",
    "        filtered_df = filtered_df.drop(filtered_df[(filtered_df.tile == i[0]) & (filtered_df.row == i[1]) & (filtered_df.col == i[2])].index)\n",
    "\n",
    "\n",
    "filtered_df.to_csv(os.path.join(project_dir,\"CSVS\",f\"PATCHES_PSEUDO_FILTERED.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37adc9e-56a8-4902-9936-30f75b79ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.6.2) SPLIT INTO TRAINING AND TESTING SETS\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "product_csv = \"PATCHES_PSEUDO_FILTERED.csv\"\n",
    "csv_path = os.path.join(project_dir,\"CSVS\",product_csv)\n",
    "df = pd.read_csv(csv_path, dtype = str)\n",
    "df['SET'] = \"EVAL\"\n",
    "\n",
    "\n",
    "df.loc[df.groupby([\"area\",\"tile\"]).sample(frac=0.8).index, 'SET'] = 'TRAIN'\n",
    "df.loc[df.query(\"SET == 'TRAIN'\").groupby([\"area\",\"tile\",\"draw\"]).sample(frac=0.1).index, 'SET'] = 'TEST'\n",
    "\n",
    "\n",
    "df.to_csv(os.path.join(project_dir,\"CSVS\",f\"PATCHES_TRAIN_TEST.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8baef8a-e3d1-4340-b6f2-c24ccf215f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B) TRAIN THE UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c580d-636b-4189-bb4d-e25b3500a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.1) CREATE A MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd7a17-03c8-40f6-8803-9524831351ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.1.1) CONFIG\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "model = \"TEST_SPOT4_NOSWIR\"\n",
    "sat = \"SPOT4\"\n",
    "area_name= \"PYR\"\n",
    "tiles = [\"30TXN\",\"30TYN\",\"31TCH\",\"31TDH\"]\n",
    "channels = [\"XS1\",\"XS2\",\"XS3\",\"HILL\",\"DEM\"]\n",
    "draws = 1\n",
    "epochs_step1 = 40\n",
    "lr_step1 = 0.01\n",
    "epochs_step2 = 200\n",
    "lr_step2 = 0.0001\n",
    "patience_step1 = 40\n",
    "patience_step2 = 200\n",
    "\n",
    "\n",
    "product_csv = \"PATCHES_TRAIN_TEST.csv\"\n",
    "csv_path = os.path.join(project_dir,\"CSVS\",product_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8350c6-de77-43eb-b9c7-bac18798616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.1.2) INITIALIZE MODEL\n",
    "\n",
    "config = {\n",
    "    \"input_csv\": csv_path,\n",
    "    \"sat\":sat,\n",
    "    \"model\":model,\n",
    "    \"area\": area_name,\n",
    "    \"channels\":channels,\n",
    "    \"draws\":draws,\n",
    "    \"tiles\":tiles,\n",
    "    \"work_dir\": project_dir,\n",
    "}\n",
    "\n",
    "with open(f\"{project_dir}/CODE/config/init_model_{model}.config\", \"w\") as outfile:\n",
    "    json.dump(config, outfile)\n",
    "\n",
    "\n",
    "jb_script=f\"{project_dir}/CODE/trex/run_unet_init_job.sh\"\n",
    "export =  \",\".join(\n",
    "    [\n",
    "        f\"config_file=\\\"{project_dir}/CODE/config/init_model_{model}.config\\\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "job_common_params =  \" \".join(\n",
    "    [\n",
    "        \"-A\",\"cnes_level2\", # cesbio or campus with max jobs 1000 and maxsubmit 10000 ; cnes_level2 with maxjobs 500 and maxsubmit 5000\n",
    "        f\" --job-name=INIT_{model}\",\n",
    "        \" --time=00:29:59\",\n",
    "        \"-N\",\"1\",\"-n\",\"8\",\n",
    "        \"--mem=184G\",\n",
    "        \"-o\",f\"{project_dir}/CODE/logs/LOG_INIT_{model}_JOB\",\n",
    "        f\"--export={export}\",jb_script\n",
    "    ]\n",
    ")\n",
    "\n",
    "os.system(\"sbatch \"+job_common_params)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3b475-0168-42dd-b818-0eade67e64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.1.3) TRAINING STEP 1: MULTIPLE \"CRUDE\" TRAININGS (high learning rate and low epoch number) \n",
    "\n",
    "jb_script=f\"{project_dir}/CODE/trex/run_unet_train_step1_job.sh\"\n",
    "seeds_per_job = 2\n",
    "seeds=10\n",
    "\n",
    "for seed in range(1,seeds+1,seeds_per_job):\n",
    "    start_seed = seed\n",
    "    end_seed = seed+seeds_per_job-1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    config = {\n",
    "        \"input_csv\": csv_path,\n",
    "        \"sat\":sat,\n",
    "        \"model\":model,\n",
    "        \"area\": area_name,\n",
    "        \"channels\":channels,\n",
    "        \"draws\":draws,\n",
    "        \"tiles\":tiles,\n",
    "        \"work_dir\": project_dir,\n",
    "        \"epochs\":epochs_step1,\n",
    "        \"learning_rate\":lr_step1,\n",
    "        \"patience\":patience_step1,\n",
    "        \"start_seed\":start_seed,\n",
    "        \"end_seed\":end_seed,\n",
    "    }\n",
    "\n",
    "    with open(f\"{project_dir}/CODE/config/step1_model_{model}_{start_seed}_{end_seed}.config\",, \"w\") as outfile:\n",
    "        json.dump(config, outfile)\n",
    "\n",
    "\n",
    "    export =  \",\".join(\n",
    "        [\n",
    "            f\"config_file=\\\"{project_dir}/CODE/config/step1_model_{model}_{start_seed}_{end_seed}.config\\\"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    job_common_params =  \" \".join(\n",
    "        [\n",
    "            \"-A\",\"cnes_level2\", # cesbio or campus with max jobs 1000 and maxsubmit 10000 ; cnes_level2 with maxjobs 500 and maxsubmit 5000\n",
    "            f\" --job-name=STEP1_{model}_{start_seed}\",\n",
    "            \" --time=05:59:59\",\n",
    "            \"-N\",\"1\",\"-n\",\"8\",\n",
    "            \"--gres=gpu:1\",\n",
    "            \"--qos=gpu_all\",\n",
    "            \"--mem=184G\",\n",
    "            \"-o\",f\"{project_dir}/CODE/logs/LOG_STEP1_{model}_SEED{start_seed}_JOB\",\n",
    "            f\"--export={export}\",jb_script\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    os.system(\"sbatch \"+job_common_params)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e1e76-7118-4dd1-8901-89e5ebd324fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.1.4) FIND BEST CHECKPOINT FROM STEP 1\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "list_df = []\n",
    "for csv in glob.glob(os.path.join(project_dir,\"MODELS\",area_name,sat,model,\"STEP1\",\"*\",\"*.csv\")):\n",
    "    list_df.append(pd.read_csv(csv))\n",
    "df = pd.concat(list_df, ignore_index = True)\n",
    "\n",
    "df = df.query(\"score_type == 'val_loss'\").sort_values(['score_value'])\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b5d99-37b6-4c0d-aac6-0970d2972300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MANUALLY PLACE THE BEST MODEL CHECKPOINT FROM STEP 1\n",
    "checkpoint_path = os.path.join(project_dir,\"MODELS\",area_name,sat,model,\"CHECKPOINT\")\n",
    "os.system(f\"mkdir -p {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd1430-251f-4ad1-b9e7-fc7c8fae59d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#B.1.5) TRAINING STEP 2: MORE INTENSIVE TRAINING (low learning rate and high epoch number) \n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "jb_script=f\"{project_dir}/CODE/trex/run_unet_train_step2_job.sh\"\n",
    "\n",
    "config = {\n",
    "    \"input_csv\": csv_path,\n",
    "    \"sat\":sat,\n",
    "    \"model\":model,\n",
    "    \"area\": area_name,\n",
    "    \"channels\":channels,\n",
    "    \"draws\":draws,\n",
    "    \"tiles\":tiles,\n",
    "    \"work_dir\": project_dir,\n",
    "    \"epochs\":epochs_step2,\n",
    "    \"learning_rate\":lr_step2,\n",
    "    \"patience\":patience_step2,\n",
    "}\n",
    "\n",
    "with open(f\"{project_dir}/CODE/config/step2_model_{model}.config\", \"w\") as outfile:\n",
    "    json.dump(config, outfile)\n",
    "\n",
    "\n",
    "export =  \",\".join(\n",
    "    [\n",
    "        f\"config_file=\\\"{project_dir}/CODE/config/step2_model_{model}.config\\\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "job_common_params =  \" \".join(\n",
    "    [\n",
    "        \"-A\",\"cnes_level2\", # cesbio or campus with max jobs 1000 and maxsubmit 10000 ; cnes_level2 with maxjobs 500 and maxsubmit 5000\n",
    "        f\" --job-name=STEP2_{model}\",\n",
    "        \" --time=05:59:59\",\n",
    "        \"-N\",\"1\",\"-n\",\"8\",\n",
    "        \"--gres=gpu:1\",\n",
    "        \"--qos=gpu_all\",\n",
    "        \"--mem=184G\",\n",
    "        \"-o\",f\"{project_dir}/CODE/logs/LOG_STEP2_{model}_JOB\",\n",
    "        f\"--export={export}\",jb_script\n",
    "    ]\n",
    ")\n",
    "\n",
    "os.system(\"sbatch \"+job_common_params)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d689c0-9db8-4afb-9551-48b12a185bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.2) MODEL EVALUATION USING PSEUDO SPOT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad47b7-5b9b-45b8-aa74-469faeef0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.2.1) INFERENCE OF EVAL PSEUDO SPOT IMAGES \n",
    "import json\n",
    "import os\n",
    "\n",
    "model = \"TEST_SPOT4_NOSWIR\"\n",
    "sat = \"SPOT4\"\n",
    "area_name= \"PYR\"\n",
    "jb_script=f\"{project_dir}/CODE/trex/run_unet_eval_job.sh\"\n",
    "\n",
    "config = {\n",
    "    \"input_csv\": csv_path,\n",
    "    \"sat\":sat,\n",
    "    \"model\":model,\n",
    "    \"area\": area_name,\n",
    "    \"channels\":channels,\n",
    "    \"draws\":draws,\n",
    "    \"tiles\":tiles,\n",
    "    \"work_dir\": project_dir,\n",
    "}\n",
    "\n",
    "with open(f\"{project_dir}/CODE/config/eval_model_{model}.config\", \"w\") as outfile:\n",
    "    json.dump(config, outfile)\n",
    "\n",
    "\n",
    "export =  \",\".join(\n",
    "    [\n",
    "        f\"config_file=\\\"{project_dir}/CODE/config/eval_model_{model}.config\\\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "job_common_params =  \" \".join(\n",
    "    [\n",
    "        \"-A\",\"cnes_level2\", # cesbio or campus with max jobs 1000 and maxsubmit 10000 ; cnes_level2 with maxjobs 500 and maxsubmit 5000\n",
    "        f\" --job-name=EVAL_{model}\",\n",
    "        \" --time=05:59:59\",\n",
    "        \"-N\",\"1\",\"-n\",\"8\",\n",
    "        \"--gres=gpu:1\",\n",
    "        \"--qos=gpu_all\",\n",
    "        \"--mem=184G\",\n",
    "        \"-o\",f\"{project_dir}/CODE/logs/LOG_EVAL_{model}_JOB\",\n",
    "        f\"--export={export}\",jb_script\n",
    "    ]\n",
    ")\n",
    "\n",
    "os.system(\"sbatch \"+job_common_params)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f693a-f9ac-4d82-9856-4045b3862e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.2.2) ANALYSIS : make dataframe\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from osgeo import osr, ogr, gdal\n",
    "from osgeo.gdalnumeric import *\n",
    "from osgeo.gdalconst import *\n",
    "import seaborn as sn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "csv_path = os.path.join(project_dir,\"MODELS\",model,area,sat,\"CSVS\",f\"PATCHES.csv\")\n",
    "df = pd.read_csv(csv_path, dtype = str)\n",
    "list_label = []\n",
    "list_pred = []\n",
    "list_hill = []\n",
    "list_mask = []\n",
    "\n",
    "x_margin , y_margin = 92, 92\n",
    "for index, row in df.iterrows():\n",
    "    hill_raster = gdal.Open(row[\"HILL\"])\n",
    "    hill_array = BandReadAsArray(hill_raster.GetRasterBand(1))\n",
    "    list_hill.append(hill_array[x_margin:-1*x_margin,y_margin:-1*y_margin])\n",
    "    \n",
    "    mask_raster = gdal.Open(row[\"MASK\"])\n",
    "    mask_array = BandReadAsArray(mask_raster.GetRasterBand(1))\n",
    "    list_mask.append(mask_array[x_margin:-1*x_margin,y_margin:-1*y_margin])\n",
    "    \n",
    "    label_raster = gdal.Open(row[\"LABEL\"])\n",
    "    label_array = BandReadAsArray(label_raster.GetRasterBand(1))\n",
    "    list_label.append(label_array[x_margin:-1*x_margin,y_margin:-1*y_margin])\n",
    "    \n",
    "    pred_raster = gdal.Open(row[\"PREDICTION\"])\n",
    "    pred_array = BandReadAsArray(pred_raster.GetRasterBand(1))\n",
    "    list_pred.append(pred_array)\n",
    "    \n",
    "    \n",
    "label = np.vstack(list_label).flatten()\n",
    "pred = np.vstack(list_pred).flatten()    \n",
    "hill = np.vstack(list_hill).flatten() \n",
    "mask = np.vstack(list_mask).flatten() \n",
    "\n",
    "df =pd.DataFrame({'PREDICTIONS': pred,\n",
    "        'REFERENCES': label,\n",
    "        'HILLSHADES': hill,\n",
    "        'MASK': mask}).query(\"MASK == 0 & REFERENCES != 3\").drop(columns=['MASK'])\n",
    "df[\"PREDICTIONS\"] = df[\"PREDICTIONS\"].replace([0, 1, 2], [\"GROUND\", \"SNOW\", \"CLOUD\"])\n",
    "df[\"REFERENCES\"] = df[\"REFERENCES\"].replace([0, 1, 2], [\"GROUND\", \"SNOW\", \"CLOUD\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b97e6e-704f-4004-9fdc-d6d552c5d18b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#B.2.3) ANALYSIS : hillshade study\n",
    "df_temp = df.melt(id_vars=\"HILLSHADES\", \n",
    "              var_name=\"CLASSIFIER\", \n",
    "              value_name=\"CLASS\")\n",
    "bins = [0,15,30,45,60,75,90,105,120,135,150,165,180,195,210,225,240,255]\n",
    "df_temp2 = df_temp.groupby([\"CLASSIFIER\",\"CLASS\",pd.cut(df_temp['HILLSHADES'], bins=bins)])[\"CLASS\"].count().to_frame(\"COUNT\").reset_index()\n",
    "df_hill_sum = df_temp.groupby([\"CLASSIFIER\",pd.cut(df_temp['HILLSHADES'], bins=bins)])[\"CLASS\"].count().to_frame(\"COUNT\").reset_index()\n",
    "df_hill_1 = df_temp2.query(\"CLASS == 'SNOW'\").reset_index()\n",
    "df_hill_1[\"PERCENTAGE\"] = df_hill_1[\"COUNT\"]/df_hill_sum[\"COUNT\"]*100\n",
    "fig, axs = plt.subplots()\n",
    "sn.barplot(ax=axs,data=df_hill_1.query(\"CLASSIFIER == 'PREDICTIONS'\"),x=\"HILLSHADES\",y=\"PERCENTAGE\",color='blue')\n",
    "sn.barplot(ax=axs,data=df_hill_1.query(\"CLASSIFIER == 'REFERENCES'\"),x=\"HILLSHADES\",y=\"PERCENTAGE\",color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2e625-c050-460c-804e-65d3f740f8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#B.2.4) ANALYSIS : make confusion matrix\n",
    "def getCM3metrics(cm11,cm12,cm13,cm21,cm22,cm23,cm31,cm32,cm33):\n",
    "    #calculation of class 1 metrics\n",
    "    TP_1 = cm11\n",
    "    TN_1 = cm22 + cm33 + cm23 + cm32\n",
    "    FP_1 = cm21 + cm31\n",
    "    FN_1 = cm12 + cm13\n",
    "    Precision_1 = TP_1/(TP_1 + FP_1)\n",
    "    Recall_1 = TP_1/(TP_1+FN_1)\n",
    "    F1_1 = 2*Precision_1*Recall_1/(Precision_1+Recall_1)\n",
    "\n",
    "    #calculation of class 2 metrics\n",
    "    TP_2 = cm22\n",
    "    TN_2 = cm11 + cm33 + cm13 + cm31\n",
    "    FP_2 = cm12 + cm32\n",
    "    FN_2 = cm21 + cm23\n",
    "    Precision_2 = TP_2/(TP_2 + FP_2)\n",
    "    Recall_2= TP_2/(TP_2+FN_2)\n",
    "    F1_2= 2*Precision_2*Recall_2/(Precision_2+Recall_2)\n",
    "\n",
    "    #calculation of class 3 metrics\n",
    "    TP_3 = cm33\n",
    "    TN_3 = cm11 + cm22 + cm12 + cm21\n",
    "    FP_3 = cm13 + cm23\n",
    "    FN_3 = cm31 + cm32\n",
    "    Precision_3= TP_3/(TP_3 + FP_3)\n",
    "    Recall_3= TP_3/(TP_3+FN_3)\n",
    "    F1_3= 2*Precision_3*Recall_3/(Precision_3+Recall_3)\n",
    "    \n",
    "    #calculation of multiclass metrics\n",
    "    total = cm11+cm12+cm13+cm21+cm22+cm23+cm31+cm32+cm33\n",
    "    Agree = (cm11+cm22+cm33)/total\n",
    "    ChanceAgree1 =(cm11+cm12+cm13)/total * (cm11+cm21+cm31)/total\n",
    "    ChanceAgree2 =(cm21+cm22+cm23)/total * (cm12+cm22+cm32)/total\n",
    "    ChanceAgree3 =(cm31+cm32+cm33)/total * (cm13+cm23+cm33)/total\n",
    "    ChanceAgree = ChanceAgree1 + ChanceAgree2 + ChanceAgree3 \n",
    "    kappa = (Agree - ChanceAgree)/(1 - ChanceAgree)\n",
    "    micro_precision = (TP_1+TP_2+TP_3)/(TP_1+TP_2+TP_3+FP_1+FP_2+FP_3)\n",
    "    micro_recall = (TP_1+TP_2+TP_3)/(TP_1+TP_2+TP_3+FN_1+FN_2+FN_3)\n",
    "    micro_F1 = 2*micro_precision*micro_recall/(micro_precision+micro_recall)\n",
    "    macro_precision = (Precision_1+Precision_2+Precision_3)/3.0\n",
    "    macro_recall = (Recall_1+Recall_2+Recall_3)/3.0\n",
    "    macro_F1 = (F1_1+F1_2+F1_3)/3.0\n",
    "    OA = Agree\n",
    "\n",
    "    return kappa, OA, [F1_1, F1_2, F1_3], [Recall_1, Recall_2, Recall_3], [Precision_1, Precision_2, Precision_3]\n",
    "\n",
    "\n",
    "\n",
    "df[\"COUNT\"]=1\n",
    "df_cm = df.groupby([\"REFERENCES\",\"PREDICTIONS\"])[\"COUNT\"].count().to_frame('COUNT').reset_index().pivot(index=\"REFERENCES\", columns=\"PREDICTIONS\", values=\"COUNT\")\n",
    "\n",
    "cmGG = df_cm.loc['GROUND', 'GROUND']\n",
    "cmGS = df_cm.loc['GROUND', 'SNOW']\n",
    "cmGC = df_cm.loc['GROUND', 'CLOUD']\n",
    "cmSG = df_cm.loc['SNOW', 'GROUND']\n",
    "cmSS = df_cm.loc['SNOW', 'SNOW']\n",
    "cmSC = df_cm.loc['SNOW', 'CLOUD']\n",
    "cmCG = df_cm.loc['CLOUD', 'GROUND']\n",
    "cmCS = df_cm.loc['CLOUD', 'SNOW']\n",
    "cmCC = df_cm.loc['CLOUD', 'CLOUD']\n",
    "print(df_cm)\n",
    "kappa,OA,(F1_G,F1_S,F1_C),(Recalls_G,Recalls_S,Recalls_C),(Precisions_G,Precisions_S,Precisions_C) = getCM3metrics(cmGG,cmGS,cmGC,cmSG,cmSS,cmSC,cmCG,cmCS,cmCC)\n",
    "\n",
    "print(kappa,OA,(F1_G,F1_S,F1_C),(Recalls_G,Recalls_S,Recalls_C),(Precisions_G,Precisions_S,Precisions_C) )\n",
    "sn.heatmap(data=df_cm,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b4bd2-04b0-4b13-9f59-0bc4d7c413a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.3) MODEL EVALUATION USING HISTORICAL SPOT DATA (PRODUCED BY SIMON GASCOIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f6ff9-ce70-4e3d-ba33-400a10b9c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.3.1) INFERENCE OF GASCOIN DATA\n",
    "import os\n",
    "simon_swh_path = os.path.join(project_dir,\"DATA/HISTORICAL_SWH_SIMON_GASCOIN\")\n",
    "refl_list = os.path.join(simon_swh_path,'LIST','list.txt')\n",
    "snow_dir = os.path.join(simon_swh_path,'L2B')\n",
    "model_dir = os.path.join(project_dir,\"MODELS_USED_FOR_UNET_PAPER/TCD-ONLY_SPOT4-NOSWIR\")\n",
    "out_dir = os.path.join(simon_swh_path,'INFERENCE')\n",
    "\n",
    "\n",
    "script_path = os.path.join(project_dir,\"CODE\",\"trex\",\"run_snow_swh_job.sh\")\n",
    "\n",
    "\n",
    "\n",
    "export =  \",\".join(\n",
    "    [\n",
    "        f\"INPUT_PATH=\\\"{refl_list}\\\"\", f\"OUTPUT_PATH=\\\"{out_dir}\\\"\", f\"MODELS_PATH=\\\"{model_dir}\\\"\", f\"KEEP_REFL=\\\"0\\\"\", f\"MASK=\\\"1\\\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "job_common_params =  \" \".join(\n",
    "    [\n",
    "        \"-A\",\"cnes_level2\",\n",
    "        \" --job-name=GASCOIN_EVAL\",\n",
    "        \" --time=00:09:59\",\n",
    "        \"-N\",\"1\",\"-n\",\"8\",\n",
    "        \"--mem=196G\",\n",
    "        \"-o\",f\"{project_dir}/CODE/logs/GASCOIN_EVAL\",\n",
    "        f\"--export=ALL,{export}\",script_path\n",
    "    ]\n",
    ")\n",
    "\n",
    "os.system(\"sbatch \"+job_common_params)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925aced1-9555-4140-bfd6-a8558025ac30",
   "metadata": {
    "tags": []
   },
   "source": [
    "<font size=\"12\">Trends in the annual snow melt-out day over the French Alps and the Pyrenees from 38 years of high resolution satellite data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56119f-fb02-400d-8614-8f337cfb4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C) PREPARE 38 YEARS OF SNOW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac683ca-74ed-40d6-93a2-04c6a78f1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C.1) CLASSIFY SWH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea382ebf-9fcc-425a-b3fd-0ae4be63371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.1.1) MAKE LISTS OF SWH DATA USING S3FS\n",
    "#use kernel s3-env\n",
    "#if s3fs generate errors, try running the command 'kinit' (followed by pwd) in the console before trying again.\n",
    "\n",
    "lists_path = os.path.join(project_dir,\"DATA\",'SWH_PRODUCTS',\"LISTS\")\n",
    "os.system(f\"mkdir -p {lists_path}\")\n",
    "\n",
    "ENDPOINT_URL=\"https://s3.datalake.cnes.fr\"\n",
    "credentials = assumerole.getCredentials(\"arn:aws:iam::732885638740:role/public-read-only-OT\", Duration=7200)\n",
    "s3 = s3fs.S3FileSystem(\n",
    "      client_kwargs={\n",
    "                      'aws_access_key_id': credentials['AWS_ACCESS_KEY_ID'],\n",
    "                \n",
    "          'aws_secret_access_key': credentials['AWS_SECRET_ACCESS_KEY'],\n",
    "                      'aws_session_token': credentials['AWS_SESSION_TOKEN'],\n",
    "         'endpoint_url': 'https://s3.datalake.cnes.fr'\n",
    "      }\n",
    "   )\n",
    "\n",
    "SPOT_tile={\n",
    "    \"ALP\":\n",
    "    {\"KMIN\":46,\n",
    "     \"KMAX\":55,\n",
    "     \"JMIN\":254,\n",
    "     \"JMAX\":263\n",
    "    },\n",
    "    \"PYR\":\n",
    "    {\"KMIN\":35,\n",
    "     \"KMAX\":48,\n",
    "     \"JMIN\":262,\n",
    "     \"JMAX\":265\n",
    "    }\n",
    "}\n",
    "for mtn in [\"PYR\",\"ALP\"]:\n",
    "    os.system(f\"mkdir -p {lists_path}/{mtn}\")\n",
    "    for sat in [\"SPOT1\",\"SPOT2\",\"SPOT3\",\"SPOT4\",\"SPOT5\"]:\n",
    "        for year in range(1986,2016):\n",
    "            for K in range(SPOT_tile[mtn][\"KMIN\"],SPOT_tile[mtn][\"KMAX\"]+1):\n",
    "                for J in range(SPOT_tile[mtn][\"JMIN\"],SPOT_tile[mtn][\"JMAX\"]+1):\n",
    "                    s3_list = s3.glob(f\"muscate/SPOTWORLDHERITAGE/{str(year)}/**/{str(sat)}*_0{str(K)}-{str(J)}-*.zip\")\n",
    "                    if len(s3_list) > 0:\n",
    "                        with open(os.path.join(lists_path,mtn,f\"{mtn}_{str(sat)}_{str(year)}.txt\"), 'w') as fp:\n",
    "                            for item in s3_list:\n",
    "                                fp.write(item+\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec3f1bd-7c6b-42ec-aab6-48cbbe04fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C.1.2) APPLY UNET ON SWH LISTS\n",
    "\n",
    "\n",
    "\n",
    "model_dir = os.path.join(project_dir,\"MODELS_USED_FOR_TRENDS/TCD-BLUE\")\n",
    "\n",
    "out_dir = os.path.join(project_dir,'DATA','SWH_PRODUCTS','INFERENCE')\n",
    "\n",
    "\n",
    "lists_path = os.path.join(project_dir,\"DATA\",'SWH_PRODUCTS',\"LISTS\")\n",
    "for i,ll in enumerate(os.listdir(lists_path)):\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    script_path = os.path.join(project_dir,\"CODE\",\"trex\",\"run_snow_swh_job.sh\")\n",
    "\n",
    "\n",
    "\n",
    "    export =  \",\".join(\n",
    "        [\n",
    "            f\"INPUT_PATH=\\\"{os.path.join(lists_path,ll)}\\\"\", f\"OUTPUT_PATH=\\\"{out_dir}\\\"\", f\"MODELS_PATH=\\\"{model_dir}\\\"\", f\"KEEP_REFL=\\\"0\\\"\", f\"MASK=\\\"1\\\"\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    job_common_params =  \" \".join(\n",
    "        [\n",
    "            \"-A\",\"cnes_level2\",\n",
    "            f\" --job-name=INF_{i}\",\n",
    "            \" --time=00:09:59\",\n",
    "            \"-N\",\"1\",\"-n\",\"8\",\n",
    "            \"--mem=196G\",\n",
    "            \"-o\",f\"{project_dir}/CODE/logs/INF_{i}\",\n",
    "            f\"--export=ALL,{export}\",script_path\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    os.system(\"sbatch \"+job_common_params)  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s3-env",
   "language": "python",
   "name": "s3-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
